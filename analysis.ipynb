{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e8d24-c606-4fc5-a9e6-8fdcddf740fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'janitor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/kam/Documents/Projects/Job/paros-bids/analysis.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kam/Documents/Projects/Job/paros-bids/analysis.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mop\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kam/Documents/Projects/Job/paros-bids/analysis.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjanitor\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mjn\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kam/Documents/Projects/Job/paros-bids/analysis.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kam/Documents/Projects/Job/paros-bids/analysis.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'janitor'"
     ]
    }
   ],
   "source": [
    "import os.path as op\n",
    "\n",
    "import janitor as jn  # noqa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_flavor as pf\n",
    "import seaborn as sns\n",
    "from mne import combine_evoked, io, read_source_estimate, spatial_src_adjacency\n",
    "from mne.stats import spatio_temporal_cluster_test, summarize_clusters_stc\n",
    "from pandas_profiling import ProfileReport\n",
    "from scipy import stats as stats\n",
    "from scipy.misc import derivative\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7de339-0b12-4801-a203-1c6b320ee5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS parameters\n",
    "pd.options.display.html.table_schema = True\n",
    "pd.options.display.max_rows = None\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "\n",
    "# %%\n",
    "@pf.register_dataframe_method\n",
    "def str_remove(df, column_name: str, pattern: str = \"\"):\n",
    "    \"\"\"Wrapper to remove string patten from a column\"\"\"\n",
    "    df[column_name] = df[column_name].str.replace(pattern, \"\")\n",
    "    return df\n",
    "\n",
    "\n",
    "@pf.register_dataframe_method\n",
    "def explode(df: pd.DataFrame, column_name: str, sep: str):\n",
    "    \"\"\"Wrapper to expand column after text processing\"\"\"\n",
    "    df[\"id\"] = df.index\n",
    "    wdf = pd.DataFrame(df[column_name].str.split(sep).fillna(\"\").tolist()).stack().reset_index()\n",
    "    # exploded_column = column_name\n",
    "    wdf.columns = [\"id\", \"depth\", column_name]  # plural form to singular form\n",
    "    # wdf[column_name] = wdf[column_name].apply(lambda x: x.strip())  # trim\n",
    "    wdf.drop(\"depth\", axis=1, inplace=True)\n",
    "\n",
    "    return pd.merge(df, wdf, on=\"id\", suffixes=(\"_drop\", \"\")).drop(\n",
    "        columns=[\"id\", column_name + \"_drop\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bad7ea-e600-46fe-967b-095ef2d0ad39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace parameters\n",
    "seed = np.random.seed(42)\n",
    "mrsi_data = \"/Users/ktavabi/Documents/Projects/paros-bids/static/all_nbwr_mrs_results_csfcorr_fits_20180417.xlsx\"\n",
    "# https://github.com/ydataai/pandas-profiling/issues/954\n",
    "study_name = \"paros-bids\"\n",
    "bids_root = \"/Volumes/LaCie/paros-bids\"\n",
    "deriv_root = f\"{bids_root}/derivatives/bids-pipeline\"\n",
    "subjects_dir = \"/Volumes/LaCie/freesurfer\"\n",
    "\n",
    "f = pd.ExcelFile(mrsi_data)\n",
    "data = f.parse(sheet_name=\"FSLcorr_metab\", header=1)\n",
    "df = data.clean_names().str_remove(\"subject\", pattern=\"sub-nbwr\")\n",
    "pivot_long_on = df.columns.values[1:]\n",
    "df = df.pivot_longer(\n",
    "    column_names=pivot_long_on, names_to=\"name\", values_to=\"value\", sort_by_appearance=True\n",
    ")\n",
    "\n",
    "df[[\"hemisphere\", \"mrsi\"]] = df.name.apply(lambda x: pd.Series(str(x).split(\"_\", 1)))\n",
    "\n",
    "df.drop(labels=[\"name\"], axis=1, inplace=True)\n",
    "df = df.reorder_columns([\"subject\", \"hemisphere\", \"mrsi\", \"value\"]).encode_categorical(\n",
    "    column_names=[\"hemisphere\", \"mrsi\"]\n",
    ")\n",
    "df[\"grp\"] = df[\"subject\"].apply(lambda x: \"asd\" if np.int16(x) < 400 else \"td\")\n",
    "df[\"hemisphere\"] = df[\"hemisphere\"].map({\"left\": \"lh\", \"right\": \"rh\"})\n",
    "df = df[df.subject != \"307\"]\n",
    "df = pd.pivot_table(\n",
    "    df, values=\"value\", index=[\"subject\", \"hemisphere\", \"grp\"], columns=[\"mrsi\"]\n",
    ").reset_index()\n",
    "df.head()\n",
    "\n",
    "subjects = df[\"subject\"].unique()\n",
    "print(subjects)\n",
    "meg_data = np.zeros((len(subjects), 2, 2, 1))  # subjects*conditions*hemisphere\n",
    "_df = jn.expand_grid(others={\"subject\": subjects, \"condition\": [1, 2], \"hemisphere\": [\"lh\", \"rh\"]})\n",
    "\n",
    "for si, subject in enumerate(subjects):\n",
    "    for ci, condition in enumerate([\"lexical\", \"nonlex\"]):\n",
    "        stc = read_source_estimate(\n",
    "            op.join(\n",
    "                deriv_root,\n",
    "                f\"sub-{subject}\",\n",
    "                \"meg\",\n",
    "                f\"sub-{subject}_task-lexicaldecision_{condition}+dSPM+morph2fsaverage+hemi-lh.stc\",\n",
    "            )\n",
    "        )\n",
    "        for hii, hem in enumerate([\"lh\", \"rh\"]):\n",
    "            _, meg_data[si, ci, hii] = stc.crop(tmin=0.150, tmax=0.500).get_peak(hemi=hem)\n",
    "\n",
    "l, m, n, r = meg_data.shape\n",
    "stc_data = meg_data.reshape(l * m * n, 1)\n",
    "stc_data = pd.DataFrame(stc_data, columns=[\"latency\"])  # unlabeled\n",
    "df_meg = pd.concat([_df, stc_data], axis=1, ignore_index=True).clean_names()\n",
    "df_meg = df_meg.rename_columns(\n",
    "    new_column_names={\"0\": \"subject\", \"1\": \"condition\", \"2\": \"hemisphere\", \"3\": \"latency\"}\n",
    ")\n",
    "df_meg.info()\n",
    "dataset = pd.merge(df, df_meg, on=[\"subject\", \"hemisphere\"], how=\"inner\")\n",
    "dataset.head()\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043d2085-d802-43f9-a877-837ff4dfc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()\n",
    "profile = ProfileReport(\n",
    "    dataset,\n",
    "    title=\"Pandas Profiling Report\",\n",
    "    vars={\"num\": {\"low_categorical_threshold\": 0}},\n",
    "    explorative=True,\n",
    ")\n",
    "profile.to_file(\"profile.html\")\n",
    "sns.pairplot(dataset, hue=\"grp\")\n",
    "dataset.columns\n",
    "\n",
    "\n",
    "# create a pipeline object\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "X = dataset[\n",
    "    [\n",
    "        \"crpluspcr\",\n",
    "        \"gaba\",\n",
    "        \"gabaovercr\",\n",
    "        \"glu_80ms\",\n",
    "        \"gluovergaba\",\n",
    "        \"gpcpluspch\",\n",
    "        \"mins\",\n",
    "        \"naaplusnaag\",\n",
    "        \"latency\",\n",
    "    ]\n",
    "].values\n",
    "X.shape\n",
    "Y = dataset[\"grp\"].map({\"asd\": 1, \"td\": 2}).values\n",
    "Y.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=seed)\n",
    "model = pipe.fit(X_train, y_train)\n",
    "model_accuracy = accuracy_score(pipe.predict(X_test), y_test)\n",
    "model_accuracy\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=seed)\n",
    "\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "feature_names = np.array(\n",
    "    [\n",
    "        \"crpluspcr\",\n",
    "        \"gaba\",\n",
    "        \"gabaovercr\",\n",
    "        \"glu_80ms\",\n",
    "        \"gluovergaba\",\n",
    "        \"gpcpluspch\",\n",
    "        \"mins\",\n",
    "        \"naaplusnaag\",\n",
    "        \"latency\",\n",
    "    ]\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "ax.boxplot(result.importances[sorted_idx].T, vert=False, labels=feature_names[sorted_idx])\n",
    "ax.set_title(\"Permutation Importance of each feature\")\n",
    "ax.set_ylabel(\"Features\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled [1].\n",
    "\n",
    "# [1] L. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5f9a3-3ec4-4327-bbfb-e551f496dfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 10))\n",
    "ax.set_title(\"Correlation Matrix\", fontsize=16)\n",
    "sns.heatmap(dataset.corr(), vmin=-1, vmax=1, cmap=\"coolwarm\", annot=True)\n",
    "for tick in ax.xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)\n",
    "    tick.label.set_rotation(90)\n",
    "for tick in ax.yaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(14)\n",
    "    tick.label.set_rotation(0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c7947-14c7-469d-966c-3c557a5dc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50080495-2e67-40c3-8093-e49db0484840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paros",
   "language": "python",
   "name": "paros"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "32ef064a9a50ccf44e3ce5f0ca7403b21db5846cbe71f0bfb95e61006c7f4cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
